{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "repbrep = '../../'\n",
    "outdir = '../../figures/'\n",
    "results_dir = '../../results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_classification_accuracies, plot_segmentation_accuracies\n",
    "import pandas as pd\n",
    "import altair_saver\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Segmentation Plots\n",
    "f360_new = pd.read_parquet(os.path.join(repbrep,'results/f360_new.parquet'))\n",
    "f360_new.dataset = 'Fusion360Seg'\n",
    "mfcad_new = pd.read_parquet(os.path.join(repbrep,'results/mfcad_new.parquet'))\n",
    "segmentation_predictions = pd.read_parquet(os.path.join(repbrep, 'results/segmentation_predictions.parquet'))\n",
    "segmentation_predictions = pd.concat([segmentation_predictions,f360_new,mfcad_new],axis=0, ignore_index=True)\n",
    "segmentation_predictions = segmentation_predictions[segmentation_predictions.model != 'Ours']\n",
    "segmentation_predictions.loc[segmentation_predictions.model == 'NewModel', 'model'] = 'Ours'\n",
    "f360_accuracy_plot = plot_segmentation_accuracies(segmentation_predictions,  'Fusion360Seg', 'macro', title='Fusion 360 Segmentation')\n",
    "mfcad_accuracy_plot = plot_segmentation_accuracies(segmentation_predictions,  'MFCAD', 'macro', title='MFCAD Segmentation')\n",
    "\n",
    "# Classification Plots\n",
    "classification_predictions = pd.read_parquet(os.path.join(repbrep, 'results/classification_predictions.parquet'))\n",
    "fabwave_new = pd.read_parquet(os.path.join(repbrep,'results/fabwave_new.parquet'))\n",
    "cpreds_idx = classification_predictions[\n",
    "    (classification_predictions.model == 'Ours') & \n",
    "    (classification_predictions.seed == 0) & \n",
    "    (classification_predictions.train_fraction == 1.0)]\n",
    "cpreds_size = classification_predictions[\n",
    "    (classification_predictions.model == 'Ours') & \n",
    "    (classification_predictions.seed == 0) & \n",
    "    (classification_predictions.test_idx == 0)]\n",
    "ommitted_dict = dict(zip(cpreds_idx.test_idx, cpreds_idx.ommitted))\n",
    "ts_dict = dict(zip(cpreds_size.train_fraction.values, cpreds_size.train_size.values))\n",
    "get_ts = np.vectorize(lambda x: ts_dict[x])\n",
    "get_ommitted = np.vectorize(lambda x : ommitted_dict[x])\n",
    "fabwave_new['train_size'] = get_ts(fabwave_new.train_fraction)\n",
    "fabwave_new['ommitted'] = get_ommitted(fabwave_new.test_idx)\n",
    "classification_predictions = pd.concat([classification_predictions,fabwave_new],axis=0, ignore_index=True)\n",
    "\n",
    "cp = classification_predictions.copy()\n",
    "cp = cp[cp.model != 'Ours']\n",
    "cp.model = np.vectorize(lambda x: 'UV-Net' if x == 'UV-Net' else 'Ours')(cp.model)\n",
    "\n",
    "cp = cp[(cp.ommitted == False)]\n",
    "cp = cp[(cp.label != 14) & (cp.label != 22)]\n",
    "\n",
    "plot_classification_accuracies(cp, 'FabWave', title='FabWave Classification')#, size_proto_model='NewModel')\n",
    "fabwave_accuracy_plot = plot_classification_accuracies(cp, 'FabWave', title='FabWave Classification')\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "altair_saver.save(f360_accuracy_plot, os.path.join(outdir, 'f360-accuracy-plot.pdf'))\n",
    "altair_saver.save(mfcad_accuracy_plot, os.path.join(outdir, 'mfcad-accuracy-plot.pdf'))\n",
    "altair_saver.save(fabwave_accuracy_plot, os.path.join(outdir, 'fabwave-accuracy-plot.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from automate import Part\n",
    "from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "f360seg_index_path = os.path.join(datasets, 'fusion360seg.json')\n",
    "f360seg_zip_path = os.path.join(datasets, 'fusion360seg.zip')\n",
    "model_checkpoint_path = os.path.join(model_dir, 'BRepFaceAutoencoder_64_1024_4.ckpt')\n",
    "computed_f360seg_codes_path = os.path.join(model_dir, 'fusion360seg_coded.pt')\n",
    "render_losses_path = os.path.join(results_dir, 'f360_render_test_losses.pt')\n",
    "figure_out_path = os.path.join(outdir, 'datasetgallery.png')\n",
    "\n",
    "train_poses_path = os.path.join(datasets, 'f360seg_test_poses.npy')\n",
    "train_zooms_path = os.path.join(datasets, 'f360seg_test_zooms.npy')\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "\n",
    "grid_density = 100\n",
    "seed = 42\n",
    "\n",
    "\n",
    "with open(f360seg_index_path,'r') as f:\n",
    "    index = json.load(f)\n",
    "data =  ZipFile(f360seg_zip_path,'r')\n",
    "parts_list = [index['template'].format(*x) for x in index['test']]\n",
    "\n",
    "np.random.seed(42)\n",
    "to_render_idx = np.random.choice(np.arange(len(parts_list)), rows*cols, replace=False)\n",
    "\n",
    "gts = []\n",
    "\n",
    "poses = np.load(train_poses_path)\n",
    "zooms = np.load(train_zooms_path)\n",
    "\n",
    "\n",
    "\n",
    "for k in tqdm(range(rows*cols)):\n",
    "    i = to_render_idx[k]\n",
    "    path = parts_list[k]\n",
    "    part = Part(data.open(path).read().decode('utf-8'))\n",
    "    pose = poses[i]\n",
    "    zoom = zooms[i]\n",
    "    ground_truth = render_part(part, pose, zoom)\n",
    "    gts.append(ground_truth)\n",
    "\n",
    "M = rows*cols\n",
    "s = size\n",
    "fig, axes = plt.subplots(int(M/cols), cols, figsize=(cols*s, s*int(M/cols)),gridspec_kw = {'wspace':0, 'hspace':0}, dpi=300)\n",
    "for i in range(M):\n",
    "    row = int(i / cols)\n",
    "    col = i % cols\n",
    "    axes[row,col].imshow(gts[i])\n",
    "    axes[row,col].axis('off')\n",
    "fig.savefig(figure_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Model Version\n",
    "import torch\n",
    "import numpy as np\n",
    "from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from automate import Part\n",
    "from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "f360seg_index_path = os.path.join(datasets, 'fusion360seg.json')\n",
    "f360seg_zip_path = os.path.join(datasets, 'fusion360seg.zip')\n",
    "model_checkpoint_path = os.path.join(model_dir, 'BRepFaceAutoencoder_64_1024_4.ckpt')\n",
    "computed_f360seg_codes_path = os.path.join(model_dir, 'fusion360seg_coded.pt')\n",
    "render_losses_path = os.path.join(results_dir, 'f360_render_test_losses.pt')\n",
    "figure_out_path = os.path.join(outdir, 'datasetgallery.png')\n",
    "\n",
    "train_poses_path = os.path.join(datasets, 'f360seg_test_poses.npy')\n",
    "train_zooms_path = os.path.join(datasets, 'f360seg_test_zooms.npy')\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "\n",
    "grid_density = 100\n",
    "seed = 42\n",
    "\n",
    "\n",
    "with open(f360seg_index_path,'r') as f:\n",
    "    index = json.load(f)\n",
    "data =  ZipFile(f360seg_zip_path,'r')\n",
    "parts_list = [index['template'].format(*x) for x in index['test']]\n",
    "\n",
    "np.random.seed(42)\n",
    "to_render_idx = np.random.choice(np.arange(len(parts_list)), rows*cols, replace=False)\n",
    "\n",
    "gts = []\n",
    "\n",
    "poses = np.load(train_poses_path)\n",
    "zooms = np.load(train_zooms_path)\n",
    "\n",
    "\n",
    "\n",
    "for k in tqdm(range(rows*cols)):\n",
    "    i = to_render_idx[k]\n",
    "    path = parts_list[k]\n",
    "    part = Part(data.open(path).read().decode('utf-8'))\n",
    "    pose = poses[i]\n",
    "    zoom = zooms[i]\n",
    "    ground_truth = render_part(part, pose, zoom)\n",
    "    gts.append(ground_truth)\n",
    "\n",
    "M = rows*cols\n",
    "s = size\n",
    "fig, axes = plt.subplots(int(M/cols), cols, figsize=(cols*s, s*int(M/cols)),gridspec_kw = {'wspace':0, 'hspace':0}, dpi=300)\n",
    "for i in range(M):\n",
    "    row = int(i / cols)\n",
    "    col = i % cols\n",
    "    axes[row,col].imshow(gts[i])\n",
    "    axes[row,col].axis('off')\n",
    "fig.savefig(figure_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index['train'])+len(index['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from hybridbrep import get_camera_angle, get_norm_factors, render_segmented_mesh, RendererParams, grid_images\n",
    "#from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "#from automate import Part\n",
    "from hybridbrep import GeneralConvEncDec, HPart, HybridPartDataset\n",
    "from automate import Part, PartOptions\n",
    "#from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "figure_out_path = os.path.join(outdir, 'reconstructions.png')\n",
    "\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "N = 100\n",
    "\n",
    "imsize=800\n",
    "\n",
    "grid_density = 100\n",
    "\n",
    "force_rendering = True\n",
    "\n",
    "print('Loading Model')\n",
    "ckpt_path = '/home/ben/Documents/research/repbrep/training_logs/reconstruction/new_with_edges/version_1/checkpoints/epoch=183-val_loss=0.002646.ckpt'\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = GeneralConvEncDec(64, 1024, 4)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "ablations_path = '../../results/recon_ablations.parquet'\n",
    "ablations = pd.read_parquet(ablations_path)\n",
    "errors = ablations[ablations.model == 'New Network, New Data']\n",
    "\n",
    "print('Loading Index')\n",
    "index_path = '../../datasets/fusion360seg.json'\n",
    "data_path = '../../datasets/fusion360seg.zip'\n",
    "with open(index_path, 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "print('Counting Faces')\n",
    "ds_test = HybridPartDataset('../../datasets/fusion360seg.json', '../../datasets/fusion360seg_hpart_fixed.zip', mode='test')\n",
    "num_faces = {i:ds_test[i].faces.shape[0] for i in range(len(ds_test))}\n",
    "\n",
    "print('Selecting Parts')\n",
    "errors['faces'] = np.vectorize(lambda x: num_faces[x])(errors.test_idx)\n",
    "errors['total_value'] = errors.value * errors.faces\n",
    "total_errors = errors.groupby('test_idx').agg({'value':sum, 'total_value':sum}).reset_index()\n",
    "total_errors['faces'] = np.vectorize(lambda x: num_faces[x])(total_errors.test_idx)\n",
    "# Selection Criteria: lowest overall amount of loss with at least 20 faces\n",
    "\n",
    "to_render_idx = total_errors[total_errors.faces > 20].sort_values('total_value',ascending=True).test_idx.values[:rows*cols]\n",
    "\n",
    "print('Rendering')\n",
    "gts = []\n",
    "renders = []\n",
    "with ZipFile(data_path, 'r') as zf:    \n",
    "    for idx in tqdm(to_render_idx):\n",
    "        key = index['template'].format(*index['test'][idx])\n",
    "        with zf.open(key, 'r') as f:\n",
    "            part_data = f.read().decode('utf-8')\n",
    "        data = ds_test[idx]\n",
    "        with torch.no_grad():\n",
    "            pred = model.grid_enc_dec(data, N)\n",
    "        V, F, C = preds_to_mesh(pred[0], N)\n",
    "        opts = PartOptions()\n",
    "        opts.set_quality = True\n",
    "        opts.quality = 0.001\n",
    "        part = Part(part_data, opts)\n",
    "\n",
    "        color_pallet = plt.get_cmap('tab20')(np.arange(num_faces[idx]))[:,:3]\n",
    "\n",
    "        camera_params = get_camera_angle(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology,\n",
    "            optimize='seg'\n",
    "        )\n",
    "\n",
    "        gt_im = render_segmented_mesh(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology, color_pallet,\n",
    "            camera_params=camera_params,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "            )\n",
    "\n",
    "        norm_center, norm_scale = get_norm_factors(data.V.numpy())\n",
    "        \n",
    "        recon_im = render_segmented_mesh(\n",
    "            V, F, C, color_pallet, camera_params=camera_params,\n",
    "            norm_center=norm_center,\n",
    "            norm_scale=norm_scale,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "        )\n",
    "\n",
    "        gts.append(gt_im)\n",
    "        renders.append(recon_im)\n",
    "\n",
    "image_rows = []\n",
    "for r in range(rows):\n",
    "    curr_row = []\n",
    "    for c in range(cols):\n",
    "        idx = r*cols+c\n",
    "        gt = gts[idx]\n",
    "        recon = renders[idx]\n",
    "        curr_row.append(gt)\n",
    "        curr_row.append(recon)\n",
    "    curr_row = np.stack(curr_row)\n",
    "    image_rows.append(curr_row)\n",
    "image_rows = np.stack(image_rows)\n",
    "\n",
    "image_grid = grid_images(image_rows)\n",
    "\n",
    "image = Image.fromarray(image_grid.astype(np.uint8))\n",
    "\n",
    "image.save(figure_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greyscale Reconstruction for Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version Using New Rendering Code\n",
    "# Problems:\n",
    "#  - normalizing parts and reconstructions separately makes them different sizes\n",
    "#    - solution: have a non-normalized version of camera param finding\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from train_latent_space import BRepFaceAutoencoder\n",
    "from rendering import render_segmented_mesh, get_camera_angle, grid_images, RendererParams\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from automate import Part, PartOptions\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "\n",
    "f360seg_index_path = os.path.join(datasets, 'fusion360seg.json')\n",
    "f360seg_zip_path = os.path.join(datasets, 'fusion360seg.zip')\n",
    "model_checkpoint_path = os.path.join(model_dir, 'BRepFaceAutoencoder_64_1024_4.ckpt')\n",
    "computed_f360seg_codes_path = os.path.join(model_dir, 'fusion360seg_coded.pt')\n",
    "render_losses_path = os.path.join(results_dir, 'f360_render_test_losses.pt')\n",
    "figure_out_path = os.path.join(outdir,'greyscale_reconstructions.png')\n",
    "\n",
    "grid_density = 100 # Point Sampling Density Per Face\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "imsize=800\n",
    "\n",
    "print('Loading and Filtering Losses')\n",
    "testing_losses = torch.load(render_losses_path)\n",
    "avg_losses = np.array([x[0].item() for x in testing_losses])\n",
    "part_sizes = np.array([x[1] for x in testing_losses])\n",
    "total_losses = avg_losses * part_sizes\n",
    "avg_sorted = sorted(enumerate(zip(avg_losses, part_sizes, total_losses)),key=lambda x: x[1][0])\n",
    "total_sorted = sorted(enumerate(zip(avg_losses, part_sizes, total_losses)),key=lambda x: x[1][2])\n",
    "total_filtered = [x for x in total_sorted if x[1][1] > 20]\n",
    "\n",
    "print('Loading Dataset')\n",
    "with open(f360seg_index_path,'r') as f:\n",
    "    index = json.load(f)\n",
    "data =  ZipFile(f360seg_zip_path,'r')\n",
    "parts_list = [index['template'].format(*x) for x in index['test']]\n",
    "\n",
    "print('Loading Model')\n",
    "model = BRepFaceAutoencoder(64,1024,4)\n",
    "ckpt = torch.load(model_checkpoint_path)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "def predict(face_codes, model, N=grid_density):\n",
    "    n_faces = face_codes.shape[0]\n",
    "    line = torch.linspace(-0.1,1.1,N)\n",
    "    grid = torch.cartesian_prod(line, line)\n",
    "    grids = grid.repeat(n_faces,1)\n",
    "    indices = torch.arange(n_faces).repeat_interleave(N*N, dim=0)\n",
    "    with torch.no_grad():\n",
    "        indexed_codes = face_codes[indices]\n",
    "        uv_codes = torch.cat([grids, indexed_codes],dim=1)\n",
    "        preds = model.decoder(uv_codes)\n",
    "    return preds\n",
    "\n",
    "codes = torch.load(computed_f360seg_codes_path)\n",
    "\n",
    "gts = []\n",
    "renders = []\n",
    "\n",
    "renderer = None\n",
    "\n",
    "options = PartOptions()\n",
    "options.set_quality = True\n",
    "options.quality = 0.001\n",
    "\n",
    "for k in tqdm(range(rows*cols), 'Predicting and Rendering'):\n",
    "    i = total_filtered[k][0]\n",
    "    N = grid_density\n",
    "    preds = predict(codes[parts_list[i]]['x'], model, N)\n",
    "    V, F, C = preds_to_mesh(preds, N)\n",
    "    part = Part(data.open(parts_list[i]).read().decode('utf-8'), options)\n",
    "    \n",
    "    camera_params = get_camera_angle(\n",
    "        part.mesh.V, \n",
    "        part.mesh.F, \n",
    "        part.mesh_topology.face_to_topology,\n",
    "        optimize='seg'\n",
    "    )\n",
    "\n",
    "    n_faces = part.mesh_topology.face_to_topology.max() + 1\n",
    "\n",
    "    face_colors = np.ones((n_faces,3)).astype(int)*150\n",
    "\n",
    "    gt_im = render_segmented_mesh(\n",
    "        part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology, face_colors,\n",
    "        camera_params=camera_params,\n",
    "        render_params=RendererParams(imsize,imsize)\n",
    "        )\n",
    "    \n",
    "    recon_im = render_segmented_mesh(\n",
    "        V, F, C, face_colors, camera_params=camera_params,\n",
    "        render_params=RendererParams(imsize,imsize)\n",
    "    )\n",
    "\n",
    "    gts.append(gt_im)\n",
    "    renders.append(recon_im)\n",
    "\n",
    "image_rows = []\n",
    "for r in range(rows):\n",
    "    curr_row = []\n",
    "    for c in range(cols):\n",
    "        idx = r*cols+c\n",
    "        gt = gts[idx]\n",
    "        recon = renders[idx]\n",
    "        curr_row.append(gt)\n",
    "        curr_row.append(recon)\n",
    "    curr_row = np.stack(curr_row)\n",
    "    image_rows.append(curr_row)\n",
    "image_rows = np.stack(image_rows)\n",
    "\n",
    "image_grid = grid_images(image_rows)\n",
    "\n",
    "Image.fromarray(image_grid.astype(np.uint8)).save(figure_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n",
      "Loading Index\n",
      "Counting Faces\n",
      "Selecting Parts\n",
      "Rendering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52996/838619694.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors['faces'] = np.vectorize(lambda x: num_faces[x])(errors.test_idx)\n",
      "/tmp/ipykernel_52996/838619694.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors['total_value'] = errors.value * errors.faces\n",
      "100%|██████████| 24/24 [03:22<00:00,  8.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from hybridbrep import get_camera_angle, get_norm_factors, render_segmented_mesh, RendererParams, grid_images\n",
    "#from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "#from automate import Part\n",
    "from hybridbrep import GeneralConvEncDec, HPart, HybridPartDataset\n",
    "from automate import Part, PartOptions\n",
    "#from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "figure_out_path = os.path.join(outdir,'greyscale_reconstructions.png')\n",
    "\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "N = 100\n",
    "\n",
    "imsize=800\n",
    "\n",
    "grid_density = 100\n",
    "\n",
    "force_rendering = True\n",
    "\n",
    "print('Loading Model')\n",
    "ckpt_path = '/home/ben/Documents/research/repbrep/training_logs/reconstruction/new_with_edges/version_1/checkpoints/epoch=183-val_loss=0.002646.ckpt'\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = GeneralConvEncDec(64, 1024, 4)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "ablations_path = '../../results/recon_ablations.parquet'\n",
    "ablations = pd.read_parquet(ablations_path)\n",
    "errors = ablations[ablations.model == 'New Network, New Data']\n",
    "\n",
    "print('Loading Index')\n",
    "index_path = '../../datasets/fusion360seg.json'\n",
    "data_path = '../../datasets/fusion360seg.zip'\n",
    "with open(index_path, 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "print('Counting Faces')\n",
    "ds_test = HybridPartDataset('../../datasets/fusion360seg.json', '../../datasets/fusion360seg_hpart_fixed.zip', mode='test')\n",
    "num_faces = {i:ds_test[i].faces.shape[0] for i in range(len(ds_test))}\n",
    "\n",
    "print('Selecting Parts')\n",
    "errors['faces'] = np.vectorize(lambda x: num_faces[x])(errors.test_idx)\n",
    "errors['total_value'] = errors.value * errors.faces\n",
    "total_errors = errors.groupby('test_idx').agg({'value':sum, 'total_value':sum}).reset_index()\n",
    "total_errors['faces'] = np.vectorize(lambda x: num_faces[x])(total_errors.test_idx)\n",
    "# Selection Criteria: lowest overall amount of loss with at least 20 faces\n",
    "\n",
    "to_render_idx = total_errors[total_errors.faces > 20].sort_values('total_value',ascending=True).test_idx.values[:rows*cols]\n",
    "\n",
    "print('Rendering')\n",
    "gts = []\n",
    "renders = []\n",
    "with ZipFile(data_path, 'r') as zf:    \n",
    "    for idx in tqdm(to_render_idx):\n",
    "        key = index['template'].format(*index['test'][idx])\n",
    "        with zf.open(key, 'r') as f:\n",
    "            part_data = f.read().decode('utf-8')\n",
    "        data = ds_test[idx]\n",
    "        with torch.no_grad():\n",
    "            pred = model.grid_enc_dec(data, N)\n",
    "        V, F, C = preds_to_mesh(pred[0], N)\n",
    "        opts = PartOptions()\n",
    "        opts.set_quality = True\n",
    "        opts.quality = 0.001\n",
    "        part = Part(part_data, opts)\n",
    "\n",
    "        n_faces = part.mesh_topology.face_to_topology.max() + 1\n",
    "\n",
    "        color_pallet = np.ones((n_faces,3)).astype(int)*150\n",
    "        \n",
    "        camera_params = get_camera_angle(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology,\n",
    "            optimize='seg'\n",
    "        )\n",
    "\n",
    "        gt_im = render_segmented_mesh(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology, color_pallet,\n",
    "            camera_params=camera_params,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "            )\n",
    "\n",
    "        norm_center, norm_scale = get_norm_factors(data.V.numpy())\n",
    "        \n",
    "        recon_im = render_segmented_mesh(\n",
    "            V, F, C, color_pallet, camera_params=camera_params,\n",
    "            norm_center=norm_center,\n",
    "            norm_scale=norm_scale,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "        )\n",
    "\n",
    "        gts.append(gt_im)\n",
    "        renders.append(recon_im)\n",
    "\n",
    "image_rows = []\n",
    "for r in range(rows):\n",
    "    curr_row = []\n",
    "    for c in range(cols):\n",
    "        idx = r*cols+c\n",
    "        gt = gts[idx]\n",
    "        recon = renders[idx]\n",
    "        curr_row.append(gt)\n",
    "        curr_row.append(recon)\n",
    "    curr_row = np.stack(curr_row)\n",
    "    image_rows.append(curr_row)\n",
    "image_rows = np.stack(image_rows)\n",
    "\n",
    "image_grid = grid_images(image_rows)\n",
    "\n",
    "image = Image.fromarray(image_grid.astype(np.uint8))\n",
    "\n",
    "image.save(figure_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nalt.data_transformers.disable_max_rows()\\n\\nlim_plot = alt.Chart(accs[accs.mse < 1]).mark_bar().encode(\\n    x=alt.X('logmse',bin=alt.Bin(maxbins=50)),\\n    y=alt.Y('sum(one)', stack='normalize', axis=alt.Axis(format='%')),\\n    color=alt.Color('label:N')\\n).facet(row='train_size')\\n\\naltair_saver.save(lim_plot, os.path.join(outdir, 'limitations.pdf'))\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import altair_saver\n",
    "\n",
    "accs = pd.read_parquet(os.path.join(results_dir, 'fusion360seg_reconstruction_vs_accuracy.parquet'))\n",
    "accs['one'] = 1\n",
    "iscorr = np.vectorize(lambda x: 'correct' if x == 1 else 'incorrect')\n",
    "accs['correct'] = iscorr(accs.label)\n",
    "accs['logmse'] = np.log(accs.mse)\n",
    "\n",
    "\"\"\"\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "lim_plot = alt.Chart(accs[accs.mse < 1]).mark_bar().encode(\n",
    "    x=alt.X('logmse',bin=alt.Bin(maxbins=50)),\n",
    "    y=alt.Y('sum(one)', stack='normalize', axis=alt.Axis(format='%')),\n",
    "    color=alt.Color('label:N')\n",
    ").facet(row='train_size')\n",
    "\n",
    "altair_saver.save(lim_plot, os.path.join(outdir, 'limitations.pdf'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import altair_saver\n",
    "\n",
    "mse_losses = torch.load('../../results/f360_recon_mse_losses.pt')\n",
    "f360_segmentation = pd.read_parquet('../../results/f360_segmentation.parquet')\n",
    "loss_records = []\n",
    "for test_idx, losses in enumerate(mse_losses):\n",
    "    for face_idx, loss in enumerate(losses):\n",
    "        loss_records.append({\n",
    "            'test_idx':test_idx,\n",
    "            'face_idx':face_idx,\n",
    "            'mse_loss':loss.item()\n",
    "        })\n",
    "mse_losses = pd.DataFrame.from_records(loss_records)\n",
    "f360_segmentation = f360_segmentation.merge(mse_losses, on=['test_idx','face_idx'])\n",
    "f360_segmentation = f360_segmentation[f360_segmentation.model == 'Ours']\n",
    "f360_segmentation = f360_segmentation[f360_segmentation.mse_loss < 10]\n",
    "bins = pd.cut(np.log(f360_segmentation.mse_loss), 10)\n",
    "bin_left = np.vectorize(lambda x: x.left)(bins)\n",
    "bin_right = np.vectorize(lambda x: x.right)(bins)\n",
    "f360_segmentation['bin_left'] = bin_left\n",
    "f360_segmentation['bin_right'] = bin_right\n",
    "binned_acc = f360_segmentation.groupby(['train_size','bin_left', 'bin_right']).agg({'accuracy':'mean'}).reset_index()\n",
    "seg_v_recon = alt.Chart(binned_acc).mark_bar().encode(\n",
    "    x = alt.X('bin_left', title='Reconstruction log mse (binned)'),\n",
    "    x2 = alt.X2('bin_right', title=None),\n",
    "    y = alt.Y('accuracy', title='Segmentation Accuracy')\n",
    ").properties(title = 'Segmentation vs Reconstruction Accuracy')\n",
    "altair_saver.save(seg_v_recon, os.path.join(outdir, 'seg_v_recon.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import render_segmentation_comparisons_newplotting\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "f360_comparisons = render_segmentation_comparisons_newplotting(root=repbrep, seg_pred_path = '../../results/f360_segmentation.parquet')\n",
    "Image.fromarray(f360_comparisons.astype(np.uint8)).save(os.path.join(outdir, 'f360seg-comparisons.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import render_segmentation_comparisons_newplotting\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "mfcad_comparisons = render_segmentation_comparisons_newplotting(\n",
    "    root=repbrep,\n",
    "    dataset_name='mfcad',\n",
    "    camera_name='mfcad',\n",
    "    dataset='MFCAD',\n",
    "    max_labels=16,\n",
    "    seg_pred_path = '../../results/mfcad_segmentation.parquet'\n",
    ")\n",
    "Image.fromarray(mfcad_comparisons.astype(np.uint8)).save(os.path.join(outdir, 'mfcad-comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rendering import render_segmented_mesh, grid_images, RendererParams\n",
    "from util import ZippedDataset\n",
    "from automate import Part\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Counted from image\n",
    "rowcol = np.array([[0,7],\n",
    "[1,11],\n",
    "[2,9],\n",
    "[4,3],\n",
    "[7,18],\n",
    "[10,2],\n",
    "[13,10],\n",
    "[15,14],\n",
    "[18,3],\n",
    "[18,15]])\n",
    "test_indices = rowcol.dot(np.array([[20],[1]])).flatten()\n",
    "\n",
    "ds = ZippedDataset(os.path.join(repbrep, 'datasets', 'fusion360seg'))\n",
    "\n",
    "paths = [ds.index['template'].format(*ds.index['test'][i]) for i in test_indices]\n",
    "parts = [Part(ds.zip.open(p,'r').read().decode('utf-8')) for p in paths]\n",
    "seg_preds = pd.read_parquet(os.path.join(results_dir, 'f360_segmentation.parquet'))\n",
    "\n",
    "seg_preds = seg_preds[\n",
    "    (seg_preds.dataset == 'Fusion360Seg') &\n",
    "    (seg_preds.model == 'Ours') &\n",
    "    (seg_preds.seed == 0) &\n",
    "    (seg_preds.train_size == 23266)\n",
    "]\n",
    "\n",
    "preds_and_labels = seg_preds.sort_values(['test_idx','face_idx']).groupby('test_idx').agg({'label':list,'prediction':list}).reset_index()\n",
    "\n",
    "preds = preds_and_labels.prediction.values[test_indices]\n",
    "labels = preds_and_labels.label.values[test_indices]\n",
    "\n",
    "color_pallet = plt.get_cmap('tab10')(np.arange(8))[:,:3]\n",
    "\n",
    "renders = []\n",
    "for part,label,pred in zip(parts,labels,preds):\n",
    "    V = part.mesh.V\n",
    "    F = part.mesh.F\n",
    "    F_id = part.mesh_topology.face_to_topology\n",
    "    l = np.array(label)\n",
    "    p = np.array(pred)\n",
    "    l = color_pallet[l]\n",
    "    p = color_pallet[p]\n",
    "    pred_im = render_segmented_mesh(V,F,F_id,p,render_params=RendererParams(800,800),camera_opt='seg')\n",
    "    label_im = render_segmented_mesh(V,F,F_id,l,render_params=RendererParams(800,800),camera_opt='seg')\n",
    "    renders.append(grid_images(np.stack([np.stack([label_im, pred_im])])))\n",
    "image_grid = grid_images(np.stack(renders).reshape((5,2,800,1600,4)))\n",
    "Image.fromarray(image_grid.astype(np.uint8)).save(os.path.join(outdir, 'segmentation_gallery.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>train_size</th>\n",
       "      <th>seed</th>\n",
       "      <th>test_idx</th>\n",
       "      <th>face_idx</th>\n",
       "      <th>prediction</th>\n",
       "      <th>label</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6615750</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615751</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615752</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615753</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615754</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668671</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>4183</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668672</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>4183</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668673</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>4183</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668674</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>4183</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668675</th>\n",
       "      <td>Fusion360Seg</td>\n",
       "      <td>Ours</td>\n",
       "      <td>23266</td>\n",
       "      <td>0</td>\n",
       "      <td>4183</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52926 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dataset model  train_size  seed  test_idx  face_idx  prediction  \\\n",
       "6615750  Fusion360Seg  Ours       23266     0         0         0           6   \n",
       "6615751  Fusion360Seg  Ours       23266     0         0         1           6   \n",
       "6615752  Fusion360Seg  Ours       23266     0         0         2           6   \n",
       "6615753  Fusion360Seg  Ours       23266     0         0         3           6   \n",
       "6615754  Fusion360Seg  Ours       23266     0         0         4           6   \n",
       "...               ...   ...         ...   ...       ...       ...         ...   \n",
       "6668671  Fusion360Seg  Ours       23266     0      4183        11           6   \n",
       "6668672  Fusion360Seg  Ours       23266     0      4183        12           6   \n",
       "6668673  Fusion360Seg  Ours       23266     0      4183        13           6   \n",
       "6668674  Fusion360Seg  Ours       23266     0      4183        14           6   \n",
       "6668675  Fusion360Seg  Ours       23266     0      4183        15           2   \n",
       "\n",
       "         label  accuracy  \n",
       "6615750      6      True  \n",
       "6615751      6      True  \n",
       "6615752      6      True  \n",
       "6615753      6      True  \n",
       "6615754      6      True  \n",
       "...        ...       ...  \n",
       "6668671      6      True  \n",
       "6668672      6      True  \n",
       "6668673      6      True  \n",
       "6668674      6      True  \n",
       "6668675      6     False  \n",
       "\n",
       "[52926 rows x 9 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRep Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automate import Part\n",
    "from rendering import render_segmented_mesh, RendererParams\n",
    "from util import arr2im\n",
    "\n",
    "part = Part(os.path.join(repbrep, 'datasets', 'figure_parts', 'philipsbolt.step'))\n",
    "V = part.mesh.V\n",
    "F = part.mesh.F\n",
    "F_id = part.mesh_topology.face_to_topology\n",
    "\n",
    "renders = []\n",
    "colors = np.stack([np.array([50,50,50,50])]*len(F_id))\n",
    "renders.append(np.stack([render_segmented_mesh(V,F,F_id,colors,transparent_bg=False)]))\n",
    "for i in tqdm(range(5)):\n",
    "    colors = np.stack([np.array([50,50,50,50])]*len(F_id))\n",
    "    colors[i] = [250,50,50,255]\n",
    "    renders.append(np.stack([render_segmented_mesh(V,F,F_id,colors,transparent_bg=False)]))\n",
    "arr2im(grid_images(np.stack(renders))).save(os.path.join(outdir, 'transparent_bolts.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import altair_saver\n",
    "\n",
    "edf = pd.read_parquet(os.path.join(results_dir, 'tf_events.parquet'))\n",
    "\n",
    "fv_edf = edf[\n",
    "    edf.metric.str.contains('val') & \n",
    "    edf.metric.str.contains('loss') & \n",
    "    (~edf.metric.str.contains('step')) &\n",
    "    (edf.dataset == 'f360seg')]\n",
    "\n",
    "def get_stop_times(x):\n",
    "    min_time = x.rel_time.values[x.value.values.argmin()]\n",
    "    max_time = x.rel_time.values[x.value.values.argmax()]\n",
    "    return pd.Series([min_time, max_time], index=['min_time','max_time'])\n",
    "stopping_times = fv_edf.groupby(\n",
    "    ['model','train_size','seed','metric']\n",
    "    ).apply(get_stop_times).reset_index()\n",
    "\n",
    "model_names = {\n",
    "    'ours':'Ours',\n",
    "    'uvnet':'UV-Net',\n",
    "    'brepnet':'BRepNet'\n",
    "}\n",
    "rename_models = np.vectorize(lambda x: model_names[x])\n",
    "stopping_times.model = rename_models(stopping_times.model)\n",
    "\n",
    "stopping_times.min_time = stopping_times.min_time / 60\n",
    "\n",
    "line = alt.Chart(stopping_times)\\\n",
    "    .mark_line()\\\n",
    "    .encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('mean(min_time)', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "band = alt.Chart(stopping_times).mark_errorband(extent='ci').encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('min_time', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "chart = (band + line).properties(title='Fusion 360 Segmentation Training Time vs Training Size')\n",
    "\n",
    "altair_saver.save(chart, os.path.join(outdir, 'f360seg-time.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import altair_saver\n",
    "\n",
    "edf = pd.read_parquet(os.path.join(results_dir, 'tf_events.parquet'))\n",
    "\n",
    "fv_edf = edf[\n",
    "    edf.metric.str.contains('val') & \n",
    "    edf.metric.str.contains('loss') & \n",
    "    (~edf.metric.str.contains('step')) &\n",
    "    (edf.dataset == 'fabwave')]\n",
    "\n",
    "def get_stop_times(x):\n",
    "    min_time = x.rel_time.values[x.value.values.argmin()]\n",
    "    max_time = x.rel_time.values[x.value.values.argmax()]\n",
    "    return pd.Series([min_time, max_time], index=['min_time','max_time'])\n",
    "stopping_times = fv_edf.groupby(\n",
    "    ['model','train_size','seed','metric']\n",
    "    ).apply(get_stop_times).reset_index()\n",
    "\n",
    "model_names = {\n",
    "    'ours':'Ours',\n",
    "    'uvnet':'UV-Net',\n",
    "    'brepnet':'BRepNet'\n",
    "}\n",
    "rename_models = np.vectorize(lambda x: model_names[x])\n",
    "stopping_times.model = rename_models(stopping_times.model)\n",
    "\n",
    "stopping_times.min_time = stopping_times.min_time / 60\n",
    "\n",
    "line = alt.Chart(stopping_times)\\\n",
    "    .mark_line()\\\n",
    "    .encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('mean(min_time)', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "band = alt.Chart(stopping_times).mark_errorband(extent='ci').encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('min_time', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "chart = (band + line).properties(title='Fusion 360 Segmentation Training Time vs Training Size')\n",
    "chart\n",
    "#altair_saver.save(chart, os.path.join(outdir, 'f360seg-time.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clipping Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybridbrep import HPart\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "N = 1000#00\n",
    "part_path = os.path.join(repbrep, 'datasets/figure_parts/philipsbolt.step')\n",
    "data = HPart(part_path, N, N, True).data\n",
    "u = data.surface_coords[0,:,0].numpy()\n",
    "v = data.surface_coords[0,:,1].numpy()\n",
    "#v = v.max() - v\n",
    "m = data.surface_samples[0,:,-1].T.numpy()\n",
    "\n",
    "((u_min, v_min), (u_max, v_max)) = data.surface_bounds[0].numpy()\n",
    "u_prime = ((u *(u_max - u_min) + u_min) - (2*np.pi))\n",
    "v_prime = (v * (v_max - v_min) + v_min)\n",
    "\n",
    "plt.scatter(u_prime,v_prime,c=(m <= 0))\n",
    "plt.colorbar()\n",
    "plt.gca().set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u_prime,v_prime,c=(m),s=.03,cmap='tab20')\n",
    "plt.colorbar()\n",
    "plt.xlim(0,2*np.pi)\n",
    "plt.ylim(-np.pi/2, np.pi/2)\n",
    "plt.gca().set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u,v,c=(m<=0),s=.2, cmap='tab20')\n",
    "plt.colorbar()\n",
    "#plt.xlim(0,2*np.pi)\n",
    "#plt.ylim(-np.pi/2, np.pi/2)\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshplot as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = data.surface_samples[0,:,:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(xyz, c=m, shading={'point_size':.03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_path_step = os.path.join(repbrep, 'datasets/frame_guide/fg1.step')\n",
    "part_path_x_t = os.path.join(repbrep, 'datasets/frame_guide/fg1.x_t')\n",
    "\n",
    "for frac in np.linspace(0,1.0,6):\n",
    "\n",
    "    data = HPart(part_path_x_t, 500, 5000, True, frac).data\n",
    "\n",
    "    u = data.surface_coords[11,:,0].numpy()\n",
    "    v = data.surface_coords[11,:,1].numpy()\n",
    "    #v = v.max() - v\n",
    "    m = data.surface_samples[11,:,-1].T.numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(u,v,c=m)\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Sampling with {frac} sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('hybridbrep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6aa2ac8926c38c90b9dbe4656179f4fe9d7aa58f584c2a3569efb38019544a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
